# To add a new cell, type '# %%'
# To add a new markdown cell, type '# %% [markdown]'
# %%
from IPython import get_ipython

null.tpl [markdown]
# # Assignment 1
# Submitted by : P1935727 Ng Zhan Kang<br>
# Class of DIT/FT/2A/11
null.tpl [markdown]
# ---
null.tpl [markdown]
# # Index
# 1. [About The RMS Titanic (Background Information)](#About-The-RMS-Titanic-(Background-Information))
# 2. [Data Importing](#Data-Importing)
# 3. [Exploratory Data Analysis](#Exploratory-Data-Analysis)
# 4. [Data Preparation](#Data-Preparation)<br>
#     4.1. [Handle missing data](#Handle-missing-data)<br>
#     4.2. [Converting Features (Feature Selection)](#Converting-Features-(Feature-Selection))<br>
#     4.3. [Correcting Data](#Correcting-Data)<br>
#     4.3. [Drop Columns](#Drop-Columns)<br>
# 5. [Model Training](#Model-Training)<br>
#     5.1. [Split Data into Train and Test Sets](#Split-Data-into-Train-and-Test-Sets)<br>
#     5.2. [Train Model with Algorithm: Logistics Regression](#Train-Model-with-Algorithm:-Logistics-Regression)<br>
#     5.3. [Train Model with Algorithm: Random Forest Classifier](#Train-Model-with-Algorithm:-Random-Forest-Classifier)<br>
#     5.4. [Train Model with Algorithm: Nu-support vector Classification (NSVC)](#Train-Model-with-Algorithm:-Nu-support-vector-Classification-(NSVC))<br>
#     5.5. [Train Model with Algorithm: Gaussian Naive Bayes](#Train-Model-with-Algorithm:-Gaussian-Naive-Bayes)<br>
# 6. [Score and Evaluate Model](#Score-and-Evaluate-Model)<br>
#     6.1. [Score Model and Evaluate Model with: Logistics Regression](#Score-Model-and-Evaluate-Model-with:-Logistics-Regression)<br>
#     6.2. [Score Model and Evaluate Model with: Random Forest Classifier](#Score-Model-and-Evaluate-Model-with:-Random-Forest-Classifier)<br>
#     6.3. [Score Model and Evaluate Model with: Nu-support vector Classification (NSVC)](#Score-Model-and-Evaluate-Model-with:-Nu-support-vector-Classification-(NSVC))<br>
#     6.4. [Score Model and Evaluate Model with: Gaussian Naive Bayes](#Score-Model-and-Evaluate-Model-with:-Gaussian-Naive-Bayes)<br>
# 7. [Submitting to Kaggle](#Submitting-to-Kaggle)<br>
#     7.1. [Stupid Baseline (Everyone Dies)](#Stupid-Baseline-(Everyone-Dies))<br>
#     7.2. [Data Preparation for the test.csv](#Data-Preparation-for-the-test.csv)<br>
#     7.3. [Prediction with Logistics Regression Trained Model](#Prediction-with-Logistics-Regression-Trained-Model)<br>
#     7.4. [Prediction with Random Forest Classifier Trained Model](#Prediction-with-Random-Forest-Classifier-Trained-Model)<br>
#     7.5. [Prediction with Nu-support vector Classification (NSVC) Trained Model](#Prediction-with-Nu-support-vector-Classification-(NSVC)-Trained-Model)<br>
#     7.6. [Prediction with Gaussian Naive Bayes Trained Model](#Prediction-with-Gaussian-Naive-Bayes-Trained-Model)<br>
# 8. [Conclusions](#Conclusions)
# 9. [References](#References)
null.tpl [markdown]
# ---
null.tpl [markdown]
# # About The RMS Titanic (Background Information)
# - BACKGROUND
#     - Owned by White Star Line, a British shipping company which was one of the most prominent shipping lines in the world providing passenger and cargo shipping between the British Empire and the United States.
#     - Was one of the 3 Olympic-class ocean liners(namely RMS Olympic, RMS Titanic and HMHS Britannic).
#     - It's original name was in fact "Number 401" and design was based on the revised design of "Olympic".
#     - Was built by the Harland and Wolff shipyard in Belfast, Northern Ireland.
#     - Was commanded by Captain Edward Smith, who would eventually sink down with the ship.
#     - Was the largest ship afloat at the time she entered service, which was 1911.
#     - Launched on 31 May 1911, laid down on 31 March 1909.
#     - Port of registry is at Liverpool, United Kingdom.
#     - Cost to build was GB£1.5 million, or SGD 266 million in SGD in 2020.
#     - Maiden voyage was on 10 April 1912.
#     - Sunk on 14 April 1912 23:40PM (Ship's time) and sank 2h 40 min later on 15 April 1912, 5 days from her maiden voyage due to a collison against an iceberg.
# 
# 
# - FEATURES
#     - Powered by steam engines and 4-cylinders, with a total power output of 34000kW.
#     - Heated by burning coal.
#     - Included a steam-driven electical plant, capable of producing more power than an average city power station of the time, and was used to provide electricity for the ship.
#     - Was notably the 'unsinkable' due to the interior of the ship subdivided into 16 primary compartments divided into 15 bulkheads extended above the waterline, which 11 vertically closing waterthight doors could seal off the compartments in the event of an   emergency.
#     - Had her own waterworks capable of heating and pumping water to all parts of the vessel via complex pipes and valves. The first class cabins were fitted with additional electical heaters.
#     - Able to accomodate 833 1st class passengers, 614 2nd class passengers and 1006 3rd class passengers. In total a capacity of 2453 passengers. In fact, her max capacity is 3547 according to most of her original configuration documents.
#     - Have a total of 20 lifeboats(14 wooden Harland & Wolff lifeboats, capacity of 65ppl),(4 Engelhardt boats, capacity of 47ppl), 2 emergency cutters(like a middle sized sailcraft) that can hold 40 ppl.
#     - Have the capacity to carry 64 lifeboats in fact, enough to carry 4000 passengers, more than her actual capacity.
#     - At that time, lifeboats were **intended to ferry survivors from a sinking ship to a rescuing ship, not keep afloat the whole population or power them to the shore.**
#     
#     
# - MAIDEN VOYAGE
#     - Liverpool was registered as her home port.
#     - Was intended to be the 1st of the many trans-Atlantic crossing between Southampton and New York via Cherbough and Queesntownon westbound runs, returning via Plymouth in England while eastbound.
#     - Had around 885 crew members on board, which are not permanent crew, which many were casual workers who only came abroad the ship a few hours before she sailed to  Southampton.
#     - Captain Edward Smith, who was transferred from the *Olympic* to take command for *Titanic*, was the ship commander.
#     - 97% of the crew were male, with 23 of them just females.
#     - Total passengers are around 1,317 people: 324 in First Class, 284 in Second Class, and 709 in Third Class. Of these, 869 (66%) were male and 447 (34%) female. There were 107 children aboard, the largest number of whom were in Third Class. The ship was considerably under capacity on her maiden voyage, as she could accommodate 2,453 passengers—833 First Class, 614 Second Class, and 1,006 Third Class.
#     - Notable passengers are American millionaire John Jacob Astor IV, who unfortunately did not survive and his wife Madeleine Force Astor, who survived the disaster.
#     - There were 3 ports that Titanic collected its passenger. The 1st port was Southampton Terminus railway station, alongside Titanic's berth, where the majority 3rd class passengers were to board, and the 1st and 2nd class passengers also boarded.
#     - The 2nd port was Cherbourg and Queenstown where additonal passengers were picked up.
#     - Titanic started her maiden voyage at noon, as scheduled.
#     
#     
# - ATLANTIC CROSSING
#     - Titanic was planned to arrive at New York Pier 59, or modern Chelsea Piers in NYC on the morning of 17 April. She travelled roughly 2643 nautical miles in total from Queenstown to Nantucket Shoals Light where she made her fatal contact with an iceberg.
#     - If Titanic were to not meet with an accident, it would only take a mere 193 nautical miles more to Ambrose Light and finally New York Harbour.
#     - There had been series or warning made by ships in the area of the Grand Banks of Newfoundland, near to where drifting ice were spotted. But the Titanic decided to still steam at full speed, which was a common practice back then. Ships were also operated close to full speed. Ice was posed to be little danger to large vessels, or at least not disastrous.
#     
#     
# - SINKING
#     - At 23:40 on April 14, Frederick Fleet, the lookout spotted a iceberg immediately ahead of Titanic and alerted the bridge. First Officer William Murdoch ordered the ship to be steered around the obstacle and the engines to be stopped, but it was too late for the starboard side of Titanic struck the iceberg, creating a series of holes below the waterline, causing water to seep in. Soon , 5 of the ship's watertight compartments were breached. It soon became clear that the ship was doomed, as she could not survive more than four compartments being flooded. Titanic began sinking bow-first, with water spilling from compartment to compartment as her angle in the water became steeper.
#     - Crew members were not prepared to handle such accidents. Titanic only had enough lifeboats to carry about half of those on board. Some lifeboats were launched only barely half-full, even if they could have rescued more passengers. Third-class passengers were largely left to fend for themselves, causing many of them to become trapped below decks as the ship filled with water.
#     - The "women and children first" protocol was generally followed when loading the lifeboats, and most of the male passengers and crew were left aboard.
#     - Distress signals were sent by wireless, rockets, and lamp, but none of the ships that responded was near enough to reach Titanic before she sank. The SS Californian, which was the last to have been in contact before the collision, saw Titanic's flares but failed to assist.
#     - A radio operator on board the Birma, for instance, estimated that it would be 6 a.m. before the liner could arrive at the scene.
#   
#   
# - SURVIVORS
#     - About 710 people survived the disaster and were conveyed by Carpathia to New York, Titanic's original destination, while at least 1,500 people lost their lives.
#     - Fewer than a third of those aboard Titanic survived the disaster. Some survivors died shortly afterwards; injuries and the effects of exposure caused the deaths of several of those brought aboard Carpathia.
#     - The last living survivor, Millvina Dean from England, who at only nine weeks old was the youngest passenger on board, died aged 97 on 31 May 2009.
#     
#     
# - GRAPHICAL IMAGE
#     <img src="https://images.squarespace-cdn.com/content/5006453fe4b09ef2252ba068/1351660113175-514SN9PXFWB9N2MNB8DV/TItanic-Survival-Infographic.jpg?format=1500w&content-type=image%2Fjpeg" alt="Graphical Image Of How Titanic Sunk" title="Title text" />
# 
null.tpl [markdown]
# ---
null.tpl [markdown]
# ## Titanic
# - For this problem, use only 1 target output variable (Survived)
# - Prepare the data and perform necessary feature engineering
# - Create **_at least two models_** 
# - Evaluate the models and pick the best candidate
# - Form some conclusions
null.tpl [markdown]
# ---
null.tpl [markdown]
# # Data Importing

# %%
# Suppress Future Warnings
import warnings
warnings.filterwarnings('ignore')


# %%
# check versions of libraries we are going to use
import sklearn
import numpy as np
import pandas as pd
import matplotlib
import platform

message="        Versions        "
print("*"*len(message))
print(message)
print("*"*len(message))
print("Scikit-learn version={}".format(sklearn.__version__))
print("Numpy version={}".format(np.__version__))
print("Pandas version={}".format(pd.__version__))
print("Matplotlib version={}".format(matplotlib.__version__))
print("Python version={}".format(platform.python_version()))


# %%
# import necessary libraries
import numpy as np
import pandas as pd
from sklearn import datasets 
from matplotlib import pyplot as plt
import seaborn as sns
import re
get_ipython().run_line_magic('matplotlib', 'inline')


# %%
# brief view of what is in the train dataset
titanic = pd.read_csv("train.csv")
titanic.head(3)


# %%
# brief view of what is in the test dataset
titanictest = pd.read_csv("test.csv")
titanictest.head(3)

null.tpl [markdown]
# <div class="alert alert-block alert-danger">
#     <b>Overview:</b> As we can see, by comparing the train dataset and test dataset, <b>survived</b> is the only missing feature in test dataset. But it is understandable because of the purpose of this assignment.
# </div>
null.tpl [markdown]
# ---
null.tpl [markdown]
# # Exploratory Data Analysis

# %%
# identify dtypes of all the features in training dataset
titanic.info()


# %%
# List out all variables with nulls/missing values
titanic.isnull().sum()


# %%
# Get list of numeric and nonnumeric variables
numvars = list(titanic.columns[titanic.dtypes != "object"])
nonnumvars = list(titanic.columns[titanic.dtypes == "object"])
print("Numvars: ", numvars)
print("Non-numvars: ", nonnumvars)


# %%
# Do some further exploration on list to get list of features used
numvars.remove('PassengerId')
numvars.remove('Survived')
numfeats = numvars
print("Numeric Features: ", numfeats)

nonnumvars.remove('Name')
nonnumvars.remove('Ticket')
nonnumfeats = nonnumvars
print("Non-numeric features: ", nonnumfeats)


# %%
# plot out a boxplot to see the age distribution in the train dataset
get_ipython().run_line_magic('matplotlib', 'inline')

import seaborn as sns
sns.set(style="whitegrid")
ax = sns.boxplot(y=titanic["Age"])


# %%
# plot out a histogram to see the highest population's age group
titanic['Age'].hist(bins=10)


# %%
# plotting out barplot to see if pclass affects survival or death rate.
sns.barplot(x='Pclass', y='Survived', data=titanic)


# %%
# plot a dist plot between sex and survivors to see the correlationship
survived = 'survived'
not_survived = 'failed to survive'
fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))
women = titanic[titanic['Sex']=='female']
men = titanic[titanic['Sex']=='male']
ax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)
ax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)
ax.legend()
ax.set_title('Female')
ax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)
ax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)
ax.legend()
_ = ax.set_title('Male')


# %%
# plot a pointplot to compare the survival rate between male and female departing from different ports.
FacetGrid = sns.FacetGrid(titanic, row='Embarked', size=4.5, aspect=1.6)
FacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )
FacetGrid.add_legend()

null.tpl [markdown]
# <div class="alert alert-block alert-danger">
#     <b>Analysis Summary:</b> As we can see, we have split the numeric features and non-numeric features apart so that we can handle the 2 datas seperatly later. We then proceed on to plot out a few graphs to see the overview of the mode age group on board Titanic, distribution plot to see correlationship between sex and survivors, bar plots to see if pclass affects survival or death rate and a few other graphs that speaks about the training dataset.
# </div>
null.tpl [markdown]
# ---
null.tpl [markdown]
# # Data Preparation
null.tpl [markdown]
# ## Handle missing data

# %%
# Replace missing age values with mean age
titanic['Age'].fillna(titanic['Age'].mean(), inplace=True)


# %%
# 2 missing values in embarked
# use the mode to replace it
titanic['Embarked'].fillna(titanic['Embarked'].mode()[0], inplace=True)
model_embarked_mode = titanic['Embarked'].mode()[0]


# %%
# check out the training dataset again
titanic.head(2)

null.tpl [markdown]
# ## Converting Features (Feature Selection)

# %%
# getting the title of passengers
def get_title(name):
    title_search = re.search(' ([A-Za-z]+)\.', name)
    # If the title exists, extract and return it.
    if title_search:
        return title_search.group(1)
    return ""
# Create a new feature Title, containing the titles of passenger names
titanic['Title'] = titanic['Name'].apply(get_title)


# %%
# extract necessary title and then change non common titles to others
dict1 = {'Dr':'Others', 'Rev':'Others', 'Col':'Others', 'Mlle':'Others', 'Major':'Others', 'Capt':'Others', 'Ms':'Others', 'Don':'Others', 'Lady':'Others', 'Countess':'Others', 'Jonkheer':'Others', 'Mme':'Others', 'Sir':'Others'}
titanic['Title'] = titanic['Title'].replace(dict1)

dict2 = {'Col':'Others', 'Rev':'Others', 'Ms':'Others', 'Dona':'Others', 'Dr':'Others'}
titanic['Title'] = titanic['Title'].replace(dict2)


# %%
# get the count of various titles
titanic['Title'].value_counts()


# %%
# For the cabin parameter there are over 600 missing values
# We will replace the Cabin value with No if missing and Yes if there is a cabin number
titanic['Cabin'].fillna('No', inplace=True)
titanic['Cabin'].replace(regex=r'^((?!No).)*$',value='Yes',inplace=True)
titanic.head(2)


# %%
titanic.info()

null.tpl [markdown]
# ## Correcting Data

# %%
# Checking the incorrect entry when age is less than 13 for male and the title is Mr.
df = titanic.loc[titanic['Title']=='Mr']
df = df.loc[df['Age']<13]
df.head()


# %%
# Correcting the entry
titanic.loc[[575],['Title']] = 'Master'

null.tpl [markdown]
# ## Drop columms

# %%
# Drop the PassengerId ana Name
titanic = titanic.drop(["PassengerId","Name","Ticket"],axis=1)
titanic.head(2)


# %%
# # Encode all the categorical variables
titanicdf = pd.get_dummies(titanic)
titanicdf.head()


# %%
titanicdf.dtypes


# %%
titanicdf.describe()


# %%
# Since all values are numeric, do a correction and sort to determine
# the most important features relative to Survived
corr = titanicdf.corr()
corr.sort_values(["Survived"], ascending = False, inplace = True)
print(corr.Survived)


# %%
# plot a colormap to see correlation
colormap = plt.cm.viridis
plt.figure(figsize=(55,15))
plt.title('Correlation between Features', y=1.05, size = 35)
sns.heatmap(titanicdf.corr(),
            linewidths=0.1, 
            vmax=1.0, 
            square=True, 
            cmap=colormap, 
            linecolor='white', 
            annot=True)

null.tpl [markdown]
# <div class="alert alert-block alert-danger">
#     <b>Analysis Summary:</b> As we can see, we correct the missing values from age feature and embarked feature. We then proceed on to do a feature selection, or in other words we created a new feature called title to identify the diferent age titles extracted from the names of the passengers. We proceed on and found out that one of the oassenger shldn't be called a Mr so we changed it to Master. Finally we proceeded to drop the columns, namely <b>PassengerId, Name</b> and <b>Ticket</b>. Afterwards, we went on to encode the non_numeric categorical variables and the do a correction to sort which features are importantly relative to Surivival of passengers.
# </div>
null.tpl [markdown]
# ---
null.tpl [markdown]
# # Model Training
null.tpl [markdown]
# ## Split Data into Train and Test Sets

# %%
# split into train dataset and test dataset from the modified titanic training set.
from sklearn.model_selection import train_test_split
y = titanicdf["Survived"].values
X = titanicdf.drop(["Survived"],axis=1).values

# add random_state=7 into the train_test_split params to control the shuffling to prevent excessive changes to overall kaggle score
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=7)


# %%
# convert X from a np.array to pd.DataFrame. we WILL be using var SIKE later to see the feature_relation for each model we use.
SIKE = titanicdf.drop(["Survived"],axis=1)

null.tpl [markdown]
# ## Train Model with Algorithm: Logistics Regression 

# %%
# do gridsearch first to identify the best params first
# estimator is RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# since our estimator is sgdregressor, we need to type out the critical params, which are min_samples_leaf, min_samples_split
# change the max_iter (default 100) accordingly. experiment it.reduce params to shorten run time
param_grid = {'max_iter': [60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90]}

# put the model
clrtestout_model = LogisticRegression() 

# fit into GridSearchCV(cv or cross validation=5 <default>. set refit=True to take the best_params_ and plug into the gridsearchCV)
clr = GridSearchCV(estimator=clrtestout_model, param_grid=param_grid, cv=5, refit=True)
clr.fit(X_train, y_train)

clr


# %%
# get the best params for RandomForestClassifier
clr.best_params_


# %%
# # identify the important features for this logistic regression (NOT APPLICABLE AS LOGISTIC REGRESSION DOES NOT HAVE feature_importances_)
# importances_df = pd.DataFrame(clr.feature_importances_, columns=['Feature_Importance'], index=SIKE.columns)
# importances_df.sort_values(by=['Feature_Importance'], ascending=False, inplace=True)
# print(importances_df)

null.tpl [markdown]
# ## Train Model with Algorithm: Random Forest Classifier

# %%
# do gridsearch first to identify the best params first
# estimator is RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# since our estimator is sgdregressor, we need to type out the critical params, which are min_samples_leaf, min_samples_split
# change the min_samples_split(defaut 2) and n_estiamtors(default 100) accordingly. experiment it.reduce params to shorten run time
param_grid = {'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15],
              'n_estimators': [25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]
             }

# put the model
crfctestout_model = RandomForestClassifier(criterion= 'entropy', max_depth= None, random_state= 0) 

# fit into GridSearchCV(cv or cross validation=5 <default>, set refit to True to apply changes)
crfc = GridSearchCV(estimator=crfctestout_model, param_grid=param_grid, cv=5, refit= True)
crfc.fit(X_train, y_train)

crfc


# %%
# get the best params for RandomForestClassifier
crfc.best_params_


# %%
# decided to set my own n_estimators and min_samples_split as it seems to work better than the recommended one.(RESTART KERNAL BEFORE USAGE)
from sklearn.ensemble import RandomForestClassifier
crfc = RandomForestClassifier(criterion= 'entropy', max_depth= None, random_state= 0, n_estimators=25, min_samples_split=10) 
crfc.fit(X_train, y_train)
crfc


# %%
# identify the important features for this random forest
importances_df = pd.DataFrame(crfc.feature_importances_, columns=['Feature_Importance'], index=SIKE.columns)
importances_df.sort_values(by=['Feature_Importance'], ascending=False, inplace=True)
print(importances_df)

null.tpl [markdown]
# ## Train Model with Algorithm: Nu-support vector Classification (NSVC)

# %%
# do gridsearch first to identify the best params first
# estimator is RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.svm import NuSVC

# since our estimator is sgdregressor, we need to type out the critical params, which are min_samples_leaf, min_samples_split
# change the gamma(defaut 'scale') and degree(default 3) accordingly. experiment it.reduce params to shorten run time
param_grid = {'gamma': [1,0.1,0.01,0.001,0.0001,0.00001,0.000001], 
              'degree': [0,1,2,3,4,5,6]
             }

# put the model
cnsvctestout_model = NuSVC() 

# fit into GridSearchCV(cv or cross validation=5 <default>)
cnsvc = GridSearchCV(estimator=cnsvctestout_model, param_grid=param_grid, cv=5, refit= True)
cnsvc.fit(X_train, y_train)

cnsvc


# %%
# get the best params for RandomForestClassifier
cnsvc.best_params_


# %%
# # identify the important features for this NullSVC (NOT APPLICABLE AS LOGISTIC REGRESSION DOES NOT HAVE feature_importances_)
# importances_df = pd.DataFrame(cnsvc.feature_importances_, columns=['Feature_Importance'], index=SIKE.columns)
# importances_df.sort_values(by=['Feature_Importance'], ascending=False, inplace=True)
# print(importances_df)

null.tpl [markdown]
# ## Train Model with Algorithm: Gaussian Naive Bayes

# %%
# Train Model with Naive Bayes
from sklearn.naive_bayes import GaussianNB
cgnb = GaussianNB()
cgnb.fit(X_train, y_train)
cgnb


# %%
# # identify the important features for this Gaussian Naive Bayes (NOT APPLICABLE AS LOGISTIC REGRESSION DOES NOT HAVE feature_importances_)
# importances_df = pd.DataFrame(cnsvc.feature_importances_, columns=['Feature_Importance'], index=SIKE.columns)
# importances_df.sort_values(by=['Feature_Importance'], ascending=False, inplace=True)
# print(importances_df)

null.tpl [markdown]
# <div class="alert alert-block alert-danger">
#     <b>Analysis Summary:</b> As we can see, we used the train_test_split model from sklearn to split the training dataset into another train and test model to feed in the respective training models. I decided to use gridsearch for sgd regressor and random tree classifier as i am confident that both will give me good results.
# </div>
null.tpl [markdown]
# ---
null.tpl [markdown]
# # Score and Evaluate Model
null.tpl [markdown]
# ## Score Model and Evaluate Model with:  Logistics Regression

# %%
# score model for test set
y_hat= clr.predict(X_test)


# %%
from sklearn.metrics import confusion_matrix
# evaluate model for test set
class_names=["Survived","Died"]
cm = confusion_matrix(y_test, y_hat, labels=[1,0])
df_cm = pd.DataFrame(cm, columns=class_names, index = class_names)
df_cm.index.name = 'Actual'
df_cm.columns.name = 'Predicted'
plt.figure(figsize = (6,5))
sns.set(font_scale=1.4)#for label size
sns.heatmap(df_cm, cmap="Blues", annot=True,annot_kws={"size": 16})# font size


# %%
# Accuracy score for test set
from sklearn.metrics import accuracy_score
score = accuracy_score(y_test, y_hat)
print("Accuracy score for the test set={:.2f}%".format(score*100))

null.tpl [markdown]
# ## Score Model and Evaluate Model with:  Random Forest Classifier

# %%
# score model for test set
y_hat = crfc.predict(X_test)


# %%
from sklearn.metrics import confusion_matrix
# evaluate model for test set
class_names=["Survived","Died"]
cm = confusion_matrix(y_test, y_hat, labels=[1,0])
df_cm = pd.DataFrame(cm, columns=class_names, index = class_names)
df_cm.index.name = 'Actual'
df_cm.columns.name = 'Predicted'
plt.figure(figsize = (6,5))
sns.set(font_scale=1.4)#for label size
sns.heatmap(df_cm, cmap="Blues", annot=True,annot_kws={"size": 16})# font size


# %%
# Accuracy score for test set
from sklearn.metrics import accuracy_score
score = accuracy_score(y_test, y_hat)
print("Accuracy score for the test set={:.2f}%".format(score*100))

null.tpl [markdown]
# ## Score Model and Evaluate Model with: Nu-support vector Classification (NuSVC)

# %%
# score model for test set
y_hat = cnsvc.predict(X_test)


# %%
from sklearn.metrics import confusion_matrix
# evaluate model for test set
class_names=["Survived","Died"]
cm = confusion_matrix(y_test, y_hat, labels=[1,0])
df_cm = pd.DataFrame(cm, columns=class_names, index = class_names)
df_cm.index.name = 'Actual'
df_cm.columns.name = 'Predicted'
plt.figure(figsize = (6,5))
sns.set(font_scale=1.4)#for label size
sns.heatmap(df_cm, cmap="Blues", annot=True,annot_kws={"size": 16})# font size


# %%
# Accuracy score for test set
from sklearn.metrics import accuracy_score
score = accuracy_score(y_test, y_hat)
print("Accuracy score for the test set={:.2f}%".format(score*100))

null.tpl [markdown]
# ## Score Model and Evaluate Model with: Gaussian Naive Bayes

# %%
# score model for test set
y_hat = cgnb.predict(X_test)


# %%
from sklearn.metrics import confusion_matrix
# evaluate model for test set
class_names=["Survived","Died"]
cm = confusion_matrix(y_test, y_hat, labels=[1,0])
df_cm = pd.DataFrame(cm, columns=class_names, index = class_names)
df_cm.index.name = 'Actual'
df_cm.columns.name = 'Predicted'
plt.figure(figsize = (6,5))
sns.set(font_scale=1.4)#for label size
sns.heatmap(df_cm, cmap="Blues", annot=True,annot_kws={"size": 16})# font size


# %%
# Accuracy score for test set
from sklearn.metrics import accuracy_score
score = accuracy_score(y_test, y_hat)
print("Accuracy score for the test set={:.2f}%".format(score*100))

null.tpl [markdown]
# <div class="alert alert-block alert-danger">
#     <b>Analysis Summary:</b> As we can see, we then generate out a score model from the test set and then plot out a confusion_matrix, which we eventually get the accuracy score for the test set.
# </div>
null.tpl [markdown]
# ---
null.tpl [markdown]
# # Submitting to Kaggle
null.tpl [markdown]
# ## Stupid Baseline (Everyone Dies)
null.tpl [markdown]
# The stupid baseline is based on the majority of *Survived* status. In which case, we will have a rule which states that everybody died in the Titanic. 

# %%
dfout = pd.DataFrame() 
dfout[["PassengerId"]] = titanictest[["PassengerId"]]
dfout["Survived"] = 0
dfout[:5]


# %%
dfout.to_csv("stupidbaseline.csv",index=False)

null.tpl [markdown]
# ## Data Preparation for the test.csv

# %%
titanictest.dtypes


# %%
titanictestdf = titanictest


# %%
# fill in the missing cabins
# We will replace the Cabin value with No if missing and Yes if there is a cabin number
titanictestdf['Cabin'].fillna('No', inplace=True)
titanictestdf['Cabin'].replace(regex=r'^((?!No).)*$',value='Yes',inplace=True)
titanictestdf.head(2)


# %%
# getting the title of passengers
def get_title(name):
    title_search = re.search(' ([A-Za-z]+)\.', name)
    # If the title exists, extract and return it.
    if title_search:
        return title_search.group(1)
    return ""
# Create a new feature Title, containing the titles of passenger names
titanictest['Title'] = titanictest['Name'].apply(get_title)


# %%
# extract necessary title and then change non common titles to others
dict1 = {'Dr':'Others', 'Rev':'Others', 'Col':'Others', 'Mlle':'Others', 'Major':'Others', 'Capt':'Others', 'Ms':'Others', 'Don':'Others', 'Lady':'Others', 'Countess':'Others', 'Jonkheer':'Others', 'Mme':'Others', 'Sir':'Others'}
titanictest['Title'] = titanictest['Title'].replace(dict1)

dict2 = {'Col':'Others', 'Rev':'Others', 'Ms':'Others', 'Dona':'Others', 'Dr':'Others'}
titanictest['Title'] = titanictest['Title'].replace(dict2)


# %%
titanictest.head()


# %%
titanictestdf = titanictestdf.drop(["PassengerId","Name","Ticket"],axis=1)
titanictestdf.head(2)


# %%
# List out all variables with nulls/missing values
titanictestdf.isnull().sum()


# %%
# fill in the missing age
titanictestdf['Age'].fillna(titanic['Age'].mean(), inplace=True)


# %%
# fill in the missing fare with the mean fare
titanictestdf['Fare'].fillna(titanic['Fare'].mean(), inplace=True)


# %%
# List out all variables with nulls/missing values
titanictestdf.isnull().sum()


# %%
# # Encode all the categorical variables
predictdf = pd.get_dummies(titanictestdf)
predictdf.head(2)


# %%
Xp = predictdf.values
Xp[:2]

null.tpl [markdown]
# ## Prediction with Logistics Regression Trained Model

# %%
yp_hat = clr.predict(Xp)


# %%
dfout = pd.DataFrame() 
dfout[["PassengerId"]] = titanictest[["PassengerId"]]
dfout["Survived"] = yp_hat
dfout[:5]


# %%
dfout.to_csv("lrpredict.csv",index=False)

null.tpl [markdown]
# ## Prediction with Random Forest Classifier Trained Model

# %%
yp_hat = crfc.predict(Xp)


# %%
dfout = pd.DataFrame() 
dfout[["PassengerId"]] = titanictest[["PassengerId"]]
dfout["Survived"] = yp_hat
dfout[:5]


# %%
dfout.to_csv("rfcpredict.csv",index=False)

null.tpl [markdown]
# ## Prediction with Nu-Support Vector Classification Trained Model

# %%
yp_hat = cnsvc.predict(Xp)


# %%
dfout = pd.DataFrame() 
dfout[["PassengerId"]] = titanictest[["PassengerId"]]
dfout["Survived"] = yp_hat
dfout[:5]


# %%
dfout.to_csv("nsvcpredict.csv",index=False)

null.tpl [markdown]
# ## Prediction with Gaussian Naive Bayes Trained Model

# %%
yp_hat = cgnb.predict(Xp)


# %%
dfout = pd.DataFrame() 
dfout[["PassengerId"]] = titanictest[["PassengerId"]]
dfout["Survived"] = yp_hat
dfout[:5]


# %%
dfout.to_csv("gnbpredict.csv",index=False)

null.tpl [markdown]
# ---
null.tpl [markdown]
# # Conclusions
null.tpl [markdown]
# -	How is your prediction task defined? And what is the meaning of the output variable?
#     - __The prediction task is defined as to predict if a passanger survive or died and the output variable is the prediction of the correct class(Survived/Dead) for the Survived Variable__
null.tpl [markdown]
# -	How do you represent your data as features?
#     - __From the test dataset, i split the varaibles(features) into numfeats and nonnumfeats to seperate numeric features away from non numeric features. For the numfeats, they are `Numeric Features:  ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']` . And for nonnumfeats, they are `Non-numeric features:  ['Sex', 'Cabin', 'Embarked']`__ <br>
null.tpl [markdown]
# -	Did you process the features in any way?
#     - __Firstly, when i recieve the data, i first group nonnumfeats and numfeats into 2 different groups. They initially are: `Numvars:  ['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']` and `Non-numvars:  ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']`. Next, i decided to drop `PassengerId` and  `Survived` from numfeats and `Name`, `Ticket` from nonnumfeats., which resulted in numfeats and nonnumfeats becoming `Numeric Features:  ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']` and `Non-numeric features:  ['Sex', 'Cabin', 'Embarked']` respectively. But during data preparation, i realised that the Title from `Name` might be relevent as it sort of tells us the age group of the person. Then, i went on to extract `Title` from `Name`.__ <br>
null.tpl [markdown]
# -	Did you bring in any additional sources of data?
#     - __No.__
null.tpl [markdown]
# -	How did you select which learning algorithms to use?
#     - __I was initially given Logistic Regression by the lecturer in the jumpstart notebook. From what i know, logistic regression is a *classification* algorithm that is used to predict the probabilty of a categorical dependent variable. In other words, the data has to be encoded first to 1 or 0 (yes or no) first before we can proceed on with using the machine learning model.__
#     - __For Random Forest Classifier, i understand it is not a *classiication* algorithm, but rather a *ensemble* algorithm. It is also a supervised learning algorithm, so suitable for this assignment. I decided to use it as it is flexible and easy to use since it can be used for both classification and regression. I understand that it is like a forest made up of trees. When there are more trees, it means that the forest is more robust. As, such when theres more nodes, that means there is more branches, and eventually more precise/finer result.__
#     - __For Nu-SVC, it is similar to the standard SVC algorithm, but it has a parameter to control the number of support vectors. It also functions like SVC, where it tries to fit a "best fit" hyperplane from the data given, dividing data in seperate groups. The name *Support Vector Classifier* gives off the hint that it is also part of the *classification* algorithm family.__
#     - __For Gaussian Naive Bayes Trained Model, or Naive Bayes Classifier, it is a feature meant to be distributed according to the Gaussian distribution, or a normal distribution. It also uses a similar method to predict probability of different class based on various attributes. Other model are used to estimate distribution of the data, but Gaussian Naive Bayes only needs to estimate the mean and the standard derivation from the training data. It is also a supervised machine learning model.__
#     - __Overall, i chose models that are supervised and easy to be used.__
#     
null.tpl [markdown]
# -	Did you try to tune the hyperparameters of the learning algorithm, and in that case how?
#     - __I used gridsearchCV to try to identify the best possible hyperparam for each model. Take for example, Linear Regression Model, i know that there is a parameter called max_iter, as such, i placed it into param_grid which i will eventually feed into gridsearchCV. In gridsearchCV, i specified the model to be trained, and cross validate it for 5 folds. I also pass a refit=True param to train the model with the recommended params.__
#     - __One exception is for Random Forest Classifier. When i tried to do a gridsearchCV for RFC, and when i tried to get the kaggle score, it does not gives me a high score. Thus, i decided to manually tweak the params myself (+5/-5) from the suggested params, and eventually got a higher kaggle score.__
null.tpl [markdown]
# -	How do you evaluate the quality of your system?
#     - __The overall quality of my system is potentially able to learn accurately from the training dataset and deliver relatively accurate result. With Random Forest Classifier as the learning algorithm, it has proved that its accuracy is higher as compared to other learning algorithm.__
null.tpl [markdown]
# -	How well does your system compare to a stupid baseline?
#     - __Overall, my system is definetly more accurate as compared to the stupid baseline. It gives a relatively high accuracy score of around 0.84 as compared to the stupid baseline's 0.50 ,using RFC as a comparison.__
null.tpl [markdown]
# -	Can you say anything about the errors that the system makes? For a classification task, you may consider a confusion matrix.
#     - __Some error that the system made was that the model can wrongly classify if the person survived or did not survive the titanic sinking and it is evident from the confusion matrix.__
null.tpl [markdown]
# -	Is it possible to say something about which features the model considers important? (Whether this is possible depends on the type of classifier you are using)
#     - __Overall, from the correction, Sex_Females have the largest correclation to Survival for 0.533797 to 1.000000. From the heatmap, we can see that Sex_Females also have the largest correlation to Survival. From the Random Forest Classifier, using the ``` feature_importance_``` param which from the documentation states that "it gets the impurity-based feature importances", gets the important feature the model detects. As we can see, Fare has the highest correlation of 0.180499.__
null.tpl [markdown]
# ---
null.tpl [markdown]
# # References
null.tpl [markdown]
# ## Background Research
# - [History Of RMS Titanic](https://en.wikipedia.org/wiki/RMS_Titanic#Features)
# - [Titanic Accident Image](https://images.squarespace-cdn.com/content/5006453fe4b09ef2252ba068/1351660113175-514SN9PXFWB9N2MNB8DV/TItanic-Survival-Infographic.jpg?format=1500w&content-type=image%2Fjpeg)
null.tpl [markdown]
# ## Tutorials
# - [Titanic Data Science Solutions Python Notebook](https://www.kaggle.com/startupsci/titanic-data-science-solutions)
# - [An Interactive Data Science Tutorial](https://www.kaggle.com/helgejo/an-interactive-data-science-tutorial)
null.tpl [markdown]
# ## Referred sources For Models And HyperParameter Tuning:
# - [Predicting The Survival Of Titanic Passengers](https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8)
# - [Interpretation And Tuning Of Hyperparameters](https://www.kaggle.com/anirudh2312/interpretation-and-tuning-of-hyperparameters)
# - [Hyperparameters Tuning](https://towardsdatascience.com/hyperparameter-tuning-c5619e7e6624)
# - [An Into To Grid Search](https://medium.com/datadriveninvestor/an-introduction-to-grid-search-ff57adcc0998)
# - [Optimal Parameters For Grid Search During Tuning](https://www.kaggle.com/viznrvn/optimal-parameters-for-svc-using-gridsearch)
# - [Checking For Correlationship](https://www.kaggle.com/jatturat/finding-important-factors-to-survive-titanic#3.-Feature-Engineering)

